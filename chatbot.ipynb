{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vishesh-Goyal7/NeuroMechs/blob/Chatbot/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install dependencies\n",
        "!pip install transformers accelerate sentencepiece pandas"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3DnyreXtceHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Import libraries\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
      ],
      "metadata": {
        "id": "SXLheeMvcfFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "login(token=\"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\")"
      ],
      "metadata": {
        "id": "oXfqmuN1jNMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "llm_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "0zzSC9l4dukr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "xo8zeNw40Us3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Load symptom-disease dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/symptomMap.csv\")  # Or upload your CSV\n",
        "all_symptoms = df.columns[:-1].tolist()"
      ],
      "metadata": {
        "id": "0_MyRzkkdKTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: LLM helper function\n",
        "def ask_llm(user_symptom_input, known_symptoms):\n",
        "    symptom_list_str = \", \".join(known_symptoms)\n",
        "    prompt = (\n",
        "    \"You are a medical chatbot. When a user tells you a symptom, \"\n",
        "    \"identify and acknowledge it. If the user says 'done', end the list.\\n\\n\"\n",
        ")\n",
        "    response = llm_pipeline(prompt, max_new_tokens=100, do_sample=True, temperature=0.7)[0]['generated_text']\n",
        "\n",
        "    # Extract only the list from the response\n",
        "    try:\n",
        "        extracted = eval(response.split(\"[\")[-1].split(\"]\")[0])\n",
        "        if isinstance(extracted, str):  # Convert single string to list\n",
        "            return [extracted.strip()]\n",
        "        return [sym.strip() for sym in extracted]\n",
        "    except:\n",
        "        return []"
      ],
      "metadata": {
        "id": "ERZSYPG5dVlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Function to get matching diseases\n",
        "def get_matching_diseases(symptoms):\n",
        "    if not symptoms:\n",
        "        return []\n",
        "    filtered = df.copy()\n",
        "    for symptom in symptoms:\n",
        "        if symptom in filtered.columns:\n",
        "            filtered = filtered[filtered[symptom] == 1]\n",
        "    # Access the disease column using its actual name from the DataFrame\n",
        "    # This could be 'Disease' or another name, check your CSV file\n",
        "    return filtered['Disease'].unique().tolist() # Changed 'disease' to 'Disease'"
      ],
      "metadata": {
        "id": "IK3QxlWvdYWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "symptom_cols = df.columns[1:]\n",
        "symptom_texts = [col.replace(\"_\", \" \").lower() for col in symptom_cols]\n",
        "\n",
        "symptom_embeddings = model.encode(symptom_texts, convert_to_tensor=True)\n",
        "\n",
        "def get_user_symptoms(user_input, top_k=5, similarity_threshold=0.4):\n",
        "    input_embedding = model.encode(user_input, convert_to_tensor=True)\n",
        "    cos_scores = util.cos_sim(input_embedding, symptom_embeddings)[0]\n",
        "    top_results = (cos_scores > similarity_threshold).nonzero()\n",
        "\n",
        "    # Check if top_results[0] is a string and convert to a list if necessary\n",
        "    indices = top_results[0]\n",
        "    if not isinstance(indices, torch.Tensor): # Check if indices is a Tensor and convert to list\n",
        "        indices = [int(indices)] if isinstance(indices, str) else [indices] # Convert string/int to list\n",
        "    else:\n",
        "        indices = indices.tolist() # Convert Tensor to list\n",
        "\n",
        "    matched_symptoms = [(symptom_cols[i], float(cos_scores[i])) for i in indices]  # Use the indices list\n",
        "    matched_symptoms.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return matched_symptoms[:top_k]"
      ],
      "metadata": {
        "id": "Zva309PEdbBc",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_symptoms(user_input, top_k=5, similarity_threshold=0.4):\n",
        "    input_embedding = model.encode(user_input, convert_to_tensor=True)\n",
        "    cos_scores = util.cos_sim(input_embedding, symptom_embeddings)[0]\n",
        "    top_results = (cos_scores > similarity_threshold).nonzero()\n",
        "\n",
        "    # Check if top_results is empty\n",
        "    if top_results.size(0) == 0:  # If top_results is empty\n",
        "        return []  # Return an empty list to indicate no matches\n",
        "\n",
        "    # Check if top_results[0] is a string and convert to a list if necessary\n",
        "    indices = top_results[0] # Access the first element only if top_results is not empty\n",
        "    if not isinstance(indices, torch.Tensor): # Check if indices is a Tensor and convert to list\n",
        "        indices = [int(indices)] if isinstance(indices, str) else [indices] # Convert string/int to list\n",
        "    else:\n",
        "        indices = indices.tolist() # Convert Tensor to list\n",
        "\n",
        "    matched_symptoms = [(symptom_cols[i], float(cos_scores[i])) for i in indices]  # Use the indices list\n",
        "    matched_symptoms.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return matched_symptoms[:top_k]"
      ],
      "metadata": {
        "id": "yYmD-MRSCmiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def match_symptom_semantic(user_input):\n",
        "    inputs = [i.strip().lower() for i in user_input.split(',')]\n",
        "    matched_symptoms = {}\n",
        "\n",
        "    for input_symptom in inputs:\n",
        "        input_embedding = model.encode(input_symptom, convert_to_tensor=True).to(symptom_embeddings.device)\n",
        "        similarities = util.cos_sim(input_embedding, symptom_embeddings)\n",
        "        best_match_idx = similarities.argmax()\n",
        "        best_match_score = similarities[0, best_match_idx].item()\n",
        "\n",
        "        if best_match_score > 0.0:\n",
        "            matched_symptoms[input_symptom] = symptom_cols[int(best_match_idx)]\n",
        "        else:\n",
        "            matched_symptoms[input_symptom] = None\n",
        "\n",
        "    return matched_symptoms"
      ],
      "metadata": {
        "id": "dLyl_sHvTJ5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def preprocess_symptoms(user_input):\n",
        "    user_input = user_input.lower()\n",
        "    user_input = re.sub(r\"\\b(i have|i'm|i am|i've got|feeling|suffering from|dealing with|experiencing|got|having)\\b\", \"\", user_input)\n",
        "    user_input = user_input.replace(\" and \", \";\").replace(\",\", \";\")\n",
        "    phrases = [p.strip().strip(\".\") for p in user_input.split(\";\") if p.strip()]\n",
        "\n",
        "    return phrases\n"
      ],
      "metadata": {
        "id": "wp1JZRDCnuQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def chatbot_interaction():\n",
        "    user_symptoms = []\n",
        "    print(\"MediBot: Hello! I'm here to help you understand your symptoms.\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"User: Enter a symptom (or type 'done' if finished): \")\n",
        "        if user_input.lower() == 'done':\n",
        "            break\n",
        "        user_symptoms.append(user_input)\n",
        "\n",
        "        # Mistral for chatbot-like response:\n",
        "        prompt = f\"MediBot: Okay, you mentioned {user_input}. Anything else?\"\n",
        "        response = llm_pipeline(prompt, max_new_tokens=50, do_sample=True, temperature=0.7)[0]['generated_text']\n",
        "        print(response)\n",
        "\n",
        "    # Save symptoms to symptoms.json\n",
        "    with open('symptoms.json', 'w') as f:\n",
        "        json.dump({\"entered_symptoms\": user_symptoms}, f, indent=4)\n",
        "\n",
        "    matches = match_symptom_semantic(\", \".join(user_symptoms))\n",
        "\n",
        "    print(\"\\nMediBot: Symptom Matches:\")\n",
        "    for k, v in matches.items():\n",
        "        if v:\n",
        "            print(f\"Input: '{k}' ➔ Matched: '{v}'\")\n",
        "        else:\n",
        "            print(f\"Input: '{k}' ➔ No good match found.\")\n",
        "\n",
        "    # Disease prediction based on matched symptoms\n",
        "    matched_symptoms_list = [v for k, v in matches.items() if v is not None]\n",
        "    predicted_diseases = get_matching_diseases(matched_symptoms_list)\n",
        "\n",
        "    if predicted_diseases:\n",
        "        print(\"\\nMediBot: Possible Diseases:\")\n",
        "        for disease in predicted_diseases:\n",
        "            print(f\"- {disease}\")\n",
        "    else:\n",
        "        print(\"\\nMediBot: No matching diseases found in the dataset.\")\n",
        "\n",
        "    # Save predicted diseases to possible_diseases.json\n",
        "    with open('possible_diseases.json', 'w') as f:\n",
        "        json.dump({\"possible_diseases\": predicted_diseases}, f, indent=4)\n",
        "\n",
        "# Run the interaction\n",
        "chatbot_interaction()"
      ],
      "metadata": {
        "id": "fGWXUFC1j8sb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}